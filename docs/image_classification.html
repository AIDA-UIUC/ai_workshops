---

title: Week 1 - Image Classification


keywords: fastai
sidebar: home_sidebar

summary: "Training a resnet to predict if an image belongs to one of $n$ clases"
description: "Training a resnet to predict if an image belongs to one of $n$ clases"
nb_path: "01_image_classification.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_image_classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Today, we're going to be "solving" a task called image classification using a model called ResNet. By the end of this workshop, you should be able to create and train your own neural networks to solve a wide variety of problems that can be forumlated as image classification.</p>
<p>But before we get into neural networks, optimizers, backprop and all that other fancy jargon, it's important to realize that what you should <em>really</em> understand when working with deep learning in the real world is <em>your</em> data.</p>
<p>Specifically, I like to think about what the inputs and outputs are. This helps me come up with concrete things to implement in code.</p>
<p>In our case, today, we're going to be working on a problem called images classification. So let's consider, at a high level, what the inputs and outputs are.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Data">The Data<a class="anchor-link" href="#The-Data"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, we know that we want to use images as the input, and do something to them. So let's focus on that bit and get a little more specific. Here are some questions you should be asking:</p>
<ol>
<li>What images are we going to use?</li>
<li>Where are they going to come from?</li>
<li>How are we going to load/store them?</li>
<li>Finally, what are we actually going to do with them?</li>
</ol>
<p>Let's answer those questions in turn, with specific code snippets as much as possible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="What-Images-to-Use?">What Images to Use?<a class="anchor-link" href="#What-Images-to-Use?"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This really isn't complicated at all: use whatever you want!</p>
<p>This notebook (and deep learning in general) makes no assumption about what your images are and what they mean to you. In the end, a computer sees an image as bunch of ones and zeros. As long as your images are in some readable format and are not corrupt, pretty much anything goes.</p>
<p>Now, one caveat is that we may not be able to make a <strong>good</strong> classifier if you pick images thaat are really hard to classify. But that's an issue that you, the person who's trying to solve the problem, should be thinking about.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Where-Am-I-Supposed-to-Get-Image-From?">Where Am I Supposed to Get Image From?<a class="anchor-link" href="#Where-Am-I-Supposed-to-Get-Image-From?"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, not too hard. Barring certain legal restrictions, you can get your images from anywhere!</p>
<p>For this particular notebook, we're going to use a dataset called Imagenette (yes, that's how it's spelled, and yes, you're supposed to say that with a French accent). Here's why:</p>
<ol>
<li>It's small. So it's fast to download and use.</li>
<li>It's based on a <strong>very</strong> popular dataset called ImageNet, which is used a lot in research.</li>
<li>There are plently of results already available, so we a lot to compare to for benchmarking ourselves.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>But...</strong> Imagenette can only take you so far. It's easy to run demo notebooks on toy datasets. What actually matters at the end of the day is if you're able to train an model on <em>your</em> data that actually works and solves your problem.</p>
<p>So, instead of running the Imagenette example like I am, what you should be doing is running this code to train a model on your own dataset.</p>
<p>You don't have be curing cancer here. The dataset could be something as simple as collection of family photos, or your favorite comic book characters. In fact, we'll also be showing you how to use a script to exract your own datset from search engine resuts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Luckily, fastai provides some nice functionality to download, extract, and load images for us, which we will be using here.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMAGENETTE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [Path(&#39;/home/iyaja/.fastai/data/imagenette2/val&#39;),Path(&#39;/home/iyaja/.fastai/data/imagenette2/train&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Imagenette dataset uses one of the most common deep learning image dataset format. It's generally refered to as the imagenet style, and it looks something like this:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One thing you'll notice is that dataset, at a high level, has been split into <code>train</code> and <code>valid</code> folders. It's important that we don't mix theme up.</p>
<p>The training set (labelled as <code>train</code>) stores the images that we'll be using to train the model, and the validation set (labelled as <code>valid</code>) is completely independant from that.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Getting-the-Data-in-A-Useable-Format">Getting the Data in A Useable Format<a class="anchor-link" href="#Getting-the-Data-in-A-Useable-Format"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the fastai datablocks api, we can easily assemble the dataset into a perfect format.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>
                 <span class="n">get_items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">,</span>
                 <span class="n">get_y</span> <span class="o">=</span> <span class="n">parent_label</span><span class="p">,</span>
                 <span class="n">splitter</span> <span class="o">=</span> <span class="n">GrandparentSplitter</span><span class="p">(</span><span class="n">valid_name</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">),</span>
                 <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.35</span><span class="p">),</span> 
                 <span class="n">batch_tfms</span> <span class="o">=</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)</span>
                <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we just created and stored in the variable <code>data</code> is a DataBlock object. In fastai, a <code>DataBlock</code> is one of the most useful tools to manage loading and handling datasets. It automates tasks like getting and loading files, splitting the dataset into training and validation sets, assigning labels to input datapoints, and applying data augmentations or tramsformations.</p>
<p>But putting that aside for a second, I'd like to draw your attention to the first parameter of the <code>DataBlock</code> contructor. It looks like this:</p>

<pre><code>blocks = (ImageBlock, CategoryBlock)</code></pre>
<p>Again, this really comes back to the idea of really understanding what your inputs and outputs are. Here, we define the "blocks" that are important in this dataset. The term "blocks" is intentionally general. It makes no assumptions about what your block is. Theoretically, it could an image, a category, a number, , text, audio, video, or anything else you want.</p>
<p>A "block" makes no assumptions about whether it is an input or output. In general, you can have any of the things I listed above be either an input or an output. By default, when we specify two blocks like we did just now, fastai assumes that the first block is the input, and the second is the output.</p>
<p>But what's important, and this really comes to the core deep learning, is that in some sense, it doesn't really matter what your blocks are. Neural nets have been called "universal function approximators" for a reason - in theory, they can learn the mapping between datapoints of <em>any</em> type.</p>
<p>So as long as you have a dataset to train on and model to do so, you can set up your datablock to solve arbitrary problems, like audio -&gt; audio, image -&gt; text, text -&gt; category, or anything else you set your mind to.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Datablock provides access to a lot of high-level functionality for us. But additionally, we have a <code>DataLoaders</code> class that provides access</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Do not confuse <code>DataLoaders</code> with <code>DataLoader</code> (the difference being an additional "s" to indicate plurality). <code>DataLoaders</code> is still a fastai class, while <code>DataLoader</code> is the PyTorch class we discussed earlier.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Among it's many useful features is the <code>show_batch</code> function that does exactly as it's name suggests.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have a way to directly access all the data we need (images and their labels) in the right format (PyTorch tensors, which are used internally by fastai), we have concluded the data preperation phase.</p>
<p>While fastai certainly makes it simpler, most of the time, it will not be this easy. Many professional data scientists claim that perparing the dataset is most time consuming task of them all. This is because we usually don't have a clean dataset like Imagenette to work with in the real world, and ethically collecting data that accurately represents the distribution of the real world is not an easy task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Model">The Model<a class="anchor-link" href="#The-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's how to train a ResNet18 on the dataset we just prepared:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.513884</td>
      <td>0.247338</td>
      <td>0.926624</td>
      <td>00:09</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.246854</td>
      <td>0.187095</td>
      <td>0.944459</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.214951</td>
      <td>0.167641</td>
      <td>0.947516</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.148322</td>
      <td>0.140674</td>
      <td>0.954904</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.100381</td>
      <td>0.134767</td>
      <td>0.956688</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.087135</td>
      <td>0.131774</td>
      <td>0.957707</td>
      <td>00:11</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While a lot of detail was abstracted away in those two lines of code. For a lot of practical purposes, that's about it.</p>
<p>The reason we can do that, is because, as I mentioned, deep learning is very general. We make no assumptions about your data. So regardless of what images we use, those two lines of code that we just ran will remain the same.</p>
<p>But what exactly is going on here? It's nice to run some code and see numbers pop up on a screen, but that's not enough to comfotably use this network to make predictions in the real world.</p>
<p>Firstly, som high-level code details: we just build a fastai <code>Learner</code>. This is the other important class you'll be using a lot (the first being the <code>DataBlock</code> + <code>DataLoaders</code> combo). A learner is a wrapper around a PyTorch model and an optimizer.</p>
<p>We'll go through these in detail in the future, but here's the quick version: a model is represented as an <code>nn.Module</code> class in PyTorch. This is what is typically referred to as a "Neural Net" pretty much everywhere on the internet. The model conatins a set of parameters. Tweaking these parameters (which are just numbers) makes the model do different things.</p>
<p>Again, to be entirely clear, let's consider what the inputs and outputs are.</p>
<p>The input to our model is the image represented as an array. So this is not a traditional image file but rather, the result of performing some preprocessing and transforms.</p>
<p>The output is a class prediction, which is another array that gives the probability of the image belonging to each class.</p>

</div>
</div>
</div>
</div>
 

